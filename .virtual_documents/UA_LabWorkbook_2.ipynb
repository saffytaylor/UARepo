











import numpy as np
import pandas as pd

#Create two arrays
years = range(2010, 2018) #creates a list of consecutive integers from 2010 to 2017
a = np.repeat(years, 4) #this uses numpy's repeat() function to repeat values
b = np.random.randint(0, 40, 32) # the randint() function can be used to generate random integers - in this case 32 values between 0 and 40
#Create data frame
c = pd.DataFrame({'a':a,'b':b})  #the curly brackets indicate a dictionary





#head returns the top five rows
c.head()





#Create a list of numbers
a = range(0, 25) #the range function generates a range of integers

b = np.array(a) #creates a one dimensional array
b = b.reshape(5,5) #create an array with 5 rows and 5 columns
b





#Multiply b by 10
b * 10


#Multiply b * b
b * b 





#Extract the 0 position row
#b[0,:]  #the colon means give all elements along that dimension

#Extract 3 position column
#b[:,3]

#Extract the 2 and 3 position columns
#b[:,2:4] # The colon is used to define a numeric vector between the two numbers

#Extract 0 and 3 position rows
#b[[0,3],:] # The list ([0,3]) is used to identify the indexes to be extracted

#Extract the value in the 2 position row and 3 position column
#b[2,3]

# If you run this cell you will get the output only from the last part of the script,
# if you want to see the outcome of every part, just copy and paste the code in another code cell





c.iloc[23,1]





c.loc[23,'b']





#Return all the values in the column called "a"
c.a


#A different way of returning the column called "a"
c["a"]


#Yet another way of returning the column called "a"
c.loc[:,"a"]





c.columns





c = c.rename(columns={'a': 'Year', 'b': 'Count'})
c.head()





# 1.
df = pd.DataFrame(columns = ['a', 'b', 'c', 'd'], index = range(100))

# 2. 
subset_df = df.iloc[:30, :3]

# 3. unsure what this is asking, returned columns at index 2 and above
filtered_df = df.iloc[:, 2:]
filtered_df.head()
#filtered_df = df.loc['b']

# 4. okay just going to fill the dataframe with random figures
#df = pd.DataFrame(np.random.randint(1,100, size=(100, 4)), columns=list('abcd'))
#df





#Read CSV file - creates a data frame called earnings
earnings = pd.read_csv("data/ACS_14_5YR_S2001_with_ann.csv")

#Show column headings
earnings.columns

#UID - Tract ID
#pop - estimated total population over 16 with income
#pop_m - estimated total population over 16 with income (margin of error)
#earnings - estimated median earnings
#earnings_m - estimated median earnings (margin of error)





earnings.info()
earnings.head()








import urllib.request
url = "https://data.london.gov.uk/download/uk-house-price-index/70ac0766-8902-4eb5-aab5-01951aaed773/UK%20House%20price%20index.xlsx"
urllib.request.urlretrieve(url, "data/UK_House_price_index.xlsx")


#Read workbook
house_price = pd.read_excel("data/UK_House_price_index.xlsx", sheet_name='Average price')
house_price





# Loading house data 
import pandas as pd
house_data = pd.read_excel("data/householdcomposition.xlsx")
pd.options.display.max_columns = None
house_data.head()


house_data.columns


house_data.info()





#Let's start by renaming some of the most important columns names.
house_data.rename(columns={'2021 super output area - middle layer':'2021_MSOA_name','Unnamed: 1': 'MSOA21CD', 'Total: All households': 'All_households', 'One-person household':'1Person_household'}, inplace=True)
house_data.head()


# for some reason my computer doesn't like changing data types of existing columns
house_data['2021_MSOA_name'] = house_data['2021_MSOA_name'].astype(str)
house_data['MSOA21CD'] = house_data['MSOA21CD'].astype(str)


# going to change names and create new columns instead
#house_data.rename(columns={'2021_MSOA_name':'OLD_2021_MSOA_name','MSOA21CD': 'OLD_MSOA21CD'}, inplace=True)

#house_data['2021_MSOA_name'] = house_data['OLD_2021_MSOA_name'].astype(str)
#house_data['MSOA21CD'] = house_data['OLD_MSOA21CD'].astype(str)


house_data.info()





#Loading geopandas
import geopandas as gpd

# Read Shapefile
shapefile_path = "data/MSOA_2021_BGC/MSOA_2021_EW_BGC_V2.shp"
gdf_shapefile = gpd.read_file(shapefile_path)


gdf_shapefile.head()


gdf_shapefile.crs


gdf_shapefile.explore()


gdf_wgs84 = gdf_shapefile.to_crs(epsg=4326)


gdf_wgs84.crs


gdf_wgs84.dtypes





keep_cols = [
    "MSOA21CD",
    "MSOA21NM",
    "geometry",
]
msoa_shp = gdf_wgs84[keep_cols]
msoa_shp.head()


msoa_shp.explore()


merged_gdf = msoa_shp.merge(house_data, on='MSOA21CD')


merged_gdf.info()


matplotlib inline


#This is a "magic", which allows figures to be rendered inside the notebook (it only needs to be run once in Notebook)
#matplotlib inline

merged_gdf.explore(column='All_households', cmap='Blues')





#Show the top rows of the geometry column
merged_gdf.geometry.head()


### Run the following line only the first time running this notebook.

#pip install leafmap 


pip install leafmap





import leafmap

m = leafmap.Map()
m


# You can use this handy web map to get the centre of the map, zoom level, and other great resources for web map development 
m = leafmap.Map(center=[54, -1], zoom=6)
m.add_gdf(merged_gdf, layer_name="Housing")
m





# Read csv into Python
data_311 = pd.read_csv("data/311.csv")
# Have a look at the structure
data_311.head()





# Create a geo data frame
from shapely.geometry import Point  # import just the Point class from the shapely package
geom = [Point(xy) for xy in zip(data_311.Lon, data_311.Lat)] #create a list of latitude, longitude pairs
SP_311 = gpd.GeoDataFrame(data_311, crs=merged_gdf.crs, geometry=geom)


# Show the results
SP_311.plot()





#Get the frequencies by the categories used within the 311 data
data_311.Category.value_counts()


# Use the loc method to extract rows from the data which relate to Sewer Issues
sewer_issues = data_311.loc[data_311.Category=="Sewer Issues", :]

# Use the square brackets "[]" to perform the same task
sewer_issues = data_311[data_311.Category=="Sewer Issues"]
sewer_issues.head()  #check out the first rows


# Extract the IDs for the "Sewer Issues"
sewer_issues_IDs = data_311.loc[data_311.Category=="Sewer Issues", "CaseID"]
sewer_issues_IDs.head()  #check out the first elements





regions_england = gpd.read_file("data/Regions_(December_2021)/RGN_DEC_2021_EN_BGC.shp")


regions_england.explore('RGN21NM')


regions_name = regions_england['RGN21NM'].value_counts()
regions_name


regions_england[regions_england.RGN21NM != "South East"].plot() # Removes South East from the plot


regions_england[regions_england.RGN21NM == "North East"].plot() # Only plots North East


regions_england = regions_england[regions_england.RGN21NM != "London"] # Overwrites the regions_england object
regions_england.plot()





#Read in coastal outline (Source from - https://www.census.gov/geo/maps-data/data/cbf/cbf_counties.html)
coast = gpd.read_file("data/cb_2015_us_county_500k.shp")
SF = gpd.read_file("data/tl_2010_06075_tract10.shp")

coast_single = coast.unary_union  #merges the US counties into a single object
SF_clipped_geoms = SF.intersection(coast_single) # Clip the SF spatial data frame object to the coastline - just returns geometries
SF_clipped = SF.copy() #make a copy of the SF dataframe
SF_clipped['geometry'] = SF_clipped_geoms #replace the old geometries with the clipped geometries
SF_clipped = SF_clipped[SF_clipped.intersects(SF_clipped_geoms.unary_union)] #subset to just the observations in the clipped area

#Plot the results
SF_clipped.plot()


SF_single = SF_clipped.unary_union  #merges SF tracts into a single object
SP_311_PIP = SP_311[SP_311.intersects(SF_single)] # Select the 331 points data that intersect with San Francisco
SP_311_PIP.plot()


SP_311_PIP.head()





#Read CSV file - creates a data frame called earnings
bachelors = pd.read_csv("data/ACS_14_5YR_S1501_with_ann.csv")
bachelors.head()

#UID - Tract ID
#Bachelor_Higher - Bachelor degree or higher %
#Bachelor_Higher_m - Bachelor degree or higher % (margin of error)





#Perform the merge
SF_Tract_ACS = pd.merge(earnings, bachelors, left_on="UID", right_on="UID")
SF_Tract_ACS = pd.merge(earnings, bachelors, on="UID") # An alternative method to the above, but a shortened version as the ID columns are the same on both data frames
#there are many more options - for more details type help(pd.merge)

#The combined data frame now looks like
SF_Tract_ACS.head() # shows the top of the data frame





#Remind yourself what the data look like...
SF_clipped.head()


SF_clipped = SF_clipped[["GEOID10", "geometry"]] #Makes a new version of the geo data frame with just the values of the GEOID10 and geometry columns

#The data frame within the data slot now looks as follows
SF_clipped.head()





SF_Tract_ACS.head() # show the top of the SF_Tract_ACS object





# Creates a new variable with a leading zero
SF_Tract_ACS['GEOID10'] = "0" + SF_Tract_ACS.UID.astype(str) #need to convert the UID column to strings before prepending the zero
SF_Tract_ACS.head()





SF_Tract_ACS.info()





SF_Tract_ACS = SF_Tract_ACS.drop('UID', axis=1)  #axis=1 indicates to drop a column (axis=0 is for rows)





#Replace the "-" and "*" characters
import numpy as np
SF_Tract_ACS.loc[SF_Tract_ACS.earnings=='-', 'earnings'] = np.nan #replace the "-" values with NA
SF_Tract_ACS.loc[SF_Tract_ACS.earnings_m=='**', 'earnings_m'] = np.nan #replace the "**" values with NA
SF_Tract_ACS.loc[SF_Tract_ACS.Bachelor_Higher=='-', 'Bachelor_Higher'] = np.nan #replace the "-" values with NA
SF_Tract_ACS.loc[SF_Tract_ACS.Bachelor_Higher_m=='**', 'Bachelor_Higher_m'] = np.nan #replace the "-" values with NA





SF_Tract_ACS.earnings = SF_Tract_ACS.earnings.astype(float)
SF_Tract_ACS.earnings_m = SF_Tract_ACS.earnings_m.astype(float)
SF_Tract_ACS.Bachelor_Higher = SF_Tract_ACS.Bachelor_Higher.astype(float)
SF_Tract_ACS.Bachelor_Higher_m = SF_Tract_ACS.Bachelor_Higher_m.astype(float)


SF_Tract_ACS.info()








SF_clipped = pd.merge(SF_clipped, SF_Tract_ACS, on="GEOID10") # merge
SF_clipped.head() #show the attribute data





SP_311_PIP.explore(tiles="CartoDB positron")





SP_311_PIP = SP_311_PIP[["Category", "geometry"]] #subset data
SP_311_PIP.head()





SF_clipped_311 = gpd.sjoin(SP_311_PIP, SF, how='inner') # point in polygon
#Cleanup the attributes
SF_clipped_311 = SF_clipped_311[["GEOID10","Category","geometry"]]
#Show the top rows of the data
SF_clipped_311.head()





#In this example, we write out a CSV file with the geo data frame SF_clipped_311
SF_clipped_311.to_csv("data/311_Tract_Coded.csv")





#This will write out a Shapefile for San Francisco - note, as the column names are a little longer than are allowed within a Shapefile and as such are automatically shortened.
SF_clipped.to_file("data/SF_clipped.shp") #the default is Shapefile, but other spatial formats are supported








import requests
import pandas as pd
import geopandas as gpd

# Let's describe the url, it is usually easier to do it like this, so in the future, you can easily update the URL
url_bikes = "https://api.glasgow.gov.uk/mobility/v1/get_rentals?startDate=2022-05-01&endDate=2023-05-01"
# Making the query to the web server, using the Get method from the requests library 
response = requests.get(url_bikes)
response





#Now we get the response from the web server, we need to translate that into a format we can manipulate, like JSON.
data = response.json()
data
# careful here you will get a huge outcome; explore what you get, and then you can clear this cell outcome


# Usually, there are two labels into the web server response the metadata, and the data; we will use the data label
# to get all attributes included. 
rental_data = data['data']
rental_data
# See the structure of the data, you can see
# 'attribute':'value' structure
# each {} define one row or one element
# Again, here you will get a huge outcome; just explore what you get, and then you can clear this cell outcome


rental_pd = pd.DataFrame(rental_data)
#Can you guess what we are doing here?
rental_pd.head()


rental_pd.shape


rental_pd.columns


# Check for NaN in the coordinates column
nan_in_column_Lat = rental_pd['startPlaceLat'].isna().any()
nan_in_column_Long = rental_pd['startPlaceLong'].isna().any()

print(nan_in_column_Lat,nan_in_column_Lat)

# Alternatively, you can use the following to count NaN values
nan_in_column_Lat = rental_pd['startPlaceLat'].isna().sum()
nan_in_column_Long = rental_pd['startPlaceLong'].isna().sum()
print(nan_in_column_Lat,nan_in_column_Lat)



clean_rental_pd = rental_pd.dropna(subset=['startPlaceLat', 'startPlaceLong', 'endPlaceLat','endPlaceLong'])
clean_rental_pd.info()





gdf_bikes_start = gpd.GeoDataFrame(clean_rental_pd, geometry=gpd.points_from_xy(clean_rental_pd['startPlaceLong'], clean_rental_pd['startPlaceLat']))
gdf_bikes_end = gpd.GeoDataFrame(clean_rental_pd, geometry=gpd.points_from_xy(clean_rental_pd['endPlaceLong'], clean_rental_pd['endPlaceLat']))

# Print the GeoDataFrame
gdf_bikes_start.info()
# Do we need all those columns? And you see, there is also a lot of pre-processing to do with all the object Dtype





gdf_bikes_start.explore()





gdf_bikes_start.crs





gdf_bikes_start = gdf_bikes_start.set_crs("EPSG:4326")


gdf_bikes_start.explore()





gdf_bikes_start.dtypes


keep_cols = [
    "startDate",
    "startPlaceId",
    "startPlaceName",
    "durationSeconds",
    "isInvalid",
    "price",
    "isEbike",
    "startPlaceLat",
    "startPlaceLong",
    "geometry",
]
gdf_bikes_start = gdf_bikes_start[keep_cols]
gdf_bikes_start.head()


gdf_bikes_start.info()





gdf_bikes_start.startPlaceId = gdf_bikes_start.startPlaceId.astype(int)
gdf_bikes_start.startPlaceName = gdf_bikes_start.startPlaceName.astype(str)
gdf_bikes_start['startDate'] = pd.to_datetime(gdf_bikes_start['startDate'], format='%Y-%m-%dT%H:%M:%SZ')


gdf_bikes_start.dtypes
#gdf_bikes_start['startPlaceName'].unique()


gdf_bikes_start.head()





from sklearn.cluster import KMeans
num_clusters = 4

kmeans_collection = KMeans(n_clusters=num_clusters, random_state=42)
gdf_bikes_start['kmeans_cluster'] = kmeans_collection.fit_predict(gdf_bikes_start[['startPlaceLong', 'startPlaceLat']])


gdf_bikes_start.head()


import leafmap



import leafmap

m = leafmap.Map(center=(55.860166, -4.257505),
                zoom=12,
                draw_control=False,
                measure_control=False,
                fullscreen_control=False,
                attribution_control=True,
                   
               )

m.add_basemap("CartoDB.Positron")
m.add_data(
    gdf_bikes_start,
    column='kmeans_cluster',
    legend_title='Clusters',
    cmap='Set1',
    k=4,
)

#Ploting the map
m








m = leafmap.Map(
    center=(56.329031,-3.798943),
    zoom=7
)
wms_url = 'https://maps.gov.scot/server/services/NRS/Census2011/MapServer/WMSServer?'
# A WMS URL include multiple layers, so you need to provide the name you need to load in your map.
# See this: https://www.spatialdata.gov.scot/geonetwork/srv/eng/catalog.search#/metadata/ff882746-e913-4f78-862e-f6e3974fb80e


m.add_wms_layer(url=wms_url, layers='WorkplaceZones2011', name='Census2011', shown=True)
m



